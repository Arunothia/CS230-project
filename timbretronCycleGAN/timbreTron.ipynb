{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Imports\n",
    "################################\n",
    "\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import random\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# configs\n",
    "################################\n",
    "\n",
    "#################\n",
    "# spectrogram\n",
    "#################\n",
    "\n",
    "# Mean of flute = -6.438913345336914\n",
    "# Median of flute = -6.118330955505371\n",
    "# stdDev of flute = 4.377377986907959\n",
    "# Max of flute = 1.8442230224609375\n",
    "# Min of flute = -39.0754280090332\n",
    "\n",
    "# Mean of piano = -6.015857219696045\n",
    "# Median of piano = -5.299488544464111\n",
    "# stdDev of piano = 4.420877456665039\n",
    "# Max of piano = 1.5825170278549194\n",
    "# Min of piano = -40.520179748535156\n",
    "\n",
    "spectrogramStats = {\n",
    "                    'flute': {'mean': -6.438913345336914, 'median': -6.118330955505371, 'stdDev': 4.377377986907959, 'max': 1.8442230224609375, 'min': -39.0754280090332},\n",
    "                    'piano': {'mean': -6.015857219696045, 'median': -5.299488544464111, 'stdDev': 4.420877456665039, 'max': 1.5825170278549194, 'min': -40.520179748535156}\n",
    "                    }\n",
    "\n",
    "# standardizationStyleOptions = normal, logNormal, uniform\n",
    "# following needs to be 'SUBTRACTED' from the data\n",
    "centerOffset = {\n",
    "                'flute': {'normal': spectrogramStats['flute']['mean'], 'logNormal': spectrogramStats['flute']['mean'], 'uniform': (spectrogramStats['flute']['min'] + spectrogramStats['flute']['max'])/2},\n",
    "                'piano': {'normal': spectrogramStats['piano']['mean'], 'logNormal': spectrogramStats['piano']['mean'], 'uniform': (spectrogramStats['piano']['min'] + spectrogramStats['piano']['max'])/2}\n",
    "                }\n",
    "\n",
    "# following needs to be 'DIVIDED' to the data\n",
    "divFactor = {\n",
    "            'flute': {'normal': 3*spectrogramStats['flute']['stdDev'], 'logNormal': (1.1*spectrogramStats['flute']['max'] - spectrogramStats['flute']['mean']) , 'uniform': 1*(spectrogramStats['flute']['max'] - spectrogramStats['flute']['min'])/2},\n",
    "            'piano': {'normal': 3*spectrogramStats['piano']['stdDev'], 'logNormal': (1.1*spectrogramStats['piano']['max'] - spectrogramStats['piano']['mean']) , 'uniform': 1*(spectrogramStats['piano']['max'] - spectrogramStats['piano']['min'])/2}\n",
    "            }\n",
    "\n",
    "\n",
    "####################\n",
    "# training params\n",
    "####################\n",
    "STANDARDIZATION_STYLE = 'uniform'\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 2\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 0.0001\n",
    "LAMBDA_CYCLE = 10\n",
    "LAMBDA_IDENTITY = 5\n",
    "GRADIENT_PENALTY = 0\n",
    "ADAM_BETA1 = 0\n",
    "ADAM_BETA2 = 0.9\n",
    "NUM_RESIDUALS = 6\n",
    "\n",
    "####################\n",
    "# checkpoint options\n",
    "####################\n",
    "SAVE_CHECKPOINTS = True\n",
    "\n",
    "####################\n",
    "# find GPU device\n",
    "####################\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (f'Device = {DEVICE}, # of CUDA devices = {torch.cuda.device_count()}')\n",
    "\n",
    "\n",
    "###################\n",
    "# add fileTag\n",
    "###################\n",
    "fileTag = f'lr_{LEARNING_RATE}_cyc_{LAMBDA_CYCLE}_id_{LAMBDA_IDENTITY}_b1_{ADAM_BETA1}_b2_{ADAM_BETA2}_numRes_{NUM_RESIDUALS}_gp_{GRADIENT_PENALTY}_stdStyle_{STANDARDIZATION_STYLE}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# discriminator\n",
    "################################\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 4, stride = stride, padding = 1, bias=True, padding_mode=\"reflect\"),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1, features=[16, 32, 64, 128], numFlatFeatures = 300):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # output shape = 168 x 128\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size = 4, stride = 2, padding = 1, bias=True, padding_mode=\"reflect\"),            \n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        layers = []   \n",
    "        in_channels = features[0]\n",
    "        \n",
    "        # linear layer will have 20 x 15 features flattened\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature, stride=2))\n",
    "            in_channels = feature        \n",
    "        \n",
    "        layers.append(nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"), \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = numFlatFeatures, out_features = 1)\n",
    "        ))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))\n",
    "        \n",
    "    \n",
    "def test():\n",
    "    x = torch.randn((5,1,336,256))\n",
    "    model = Discriminator(in_channels=1)\n",
    "    print (model)\n",
    "    preds = model(x)\n",
    "    print(preds.shape)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# generator\n",
    "################################\n",
    "\n",
    "class EncoderConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_act=True, use_tanh = False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoderConv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            (nn.Tanh() if use_tanh else nn.ReLU(inplace=True)) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoderConv(x)\n",
    "\n",
    "\n",
    "class DecoderConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_act=True, use_tanh = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nearest neighbor upsample + same convolution\n",
    "        self.decoderConv = nn.Sequential(            \n",
    "            nn.Upsample(scale_factor = 2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            (nn.Tanh() if use_tanh else nn.ReLU(inplace=True)) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoderConv(x)\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_tanh = False):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            EncoderConvBlock(channels, channels, use_act = True,  use_tanh = use_tanh, kernel_size=3, stride = 1, padding=1),\n",
    "            EncoderConvBlock(channels, channels, use_act = False, use_tanh = use_tanh, kernel_size=3, stride = 1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_channels, use_tanh = False, num_features = 16, num_residuals=6):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # same convolution. output channels = 16\n",
    "        self.initial = nn.Sequential(            \n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                # output shape = floor((W - F + 2P) / S) + 1 . \n",
    "                # Wout x Hout = 128 x 168, for Win x Hin = 256 x 336, output channels = 32\n",
    "                EncoderConvBlock(num_features, num_features*2, use_act = True, use_tanh = use_tanh, kernel_size=3, stride=2, padding=1),\n",
    "                \n",
    "                # Wout x Hout = 64 x 84, for Win x Hin = 128 x 168, output channels = 64\n",
    "                EncoderConvBlock(num_features*2, num_features*4, use_act = True, use_tanh = use_tanh, kernel_size=3, stride=2, padding=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # same convolutions in residual blocks. Shape remains 64 x 86, numChannels = 64\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4, use_tanh = use_tanh) for _ in range(num_residuals)]\n",
    "        )\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                # Wout x Hout = 128 x 168, numChannels = 32\n",
    "                DecoderConvBlock(num_features*4, num_features*2, use_act = True, use_tanh = use_tanh),\n",
    "                \n",
    "                # Wout x Hout = 256 x 336, numChannels = 16\n",
    "                DecoderConvBlock(num_features*2, num_features, use_act = True, use_tanh = use_tanh)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # same convolution, numChannels = 1\n",
    "        self.last = nn.Conv2d(num_features, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # same convolution\n",
    "        x = self.initial(x)\n",
    "        \n",
    "        # down sampling\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "            \n",
    "        # same convolutions\n",
    "        x = self.residual_blocks(x)\n",
    "        \n",
    "        # upsampling\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # same convolution\n",
    "        x = self.last(x)\n",
    "        \n",
    "        return torch.tanh(x)\n",
    "\n",
    "    \n",
    "def test():\n",
    "    x = torch.randn((5,1,336,256))\n",
    "    model = Generator(img_channels=1, num_features = 16, num_residuals=9)\n",
    "    print (model)\n",
    "    gen = model(x)    \n",
    "    print(gen.shape)\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, rootDir):\n",
    "        'Initialization'\n",
    "        self.rootDir = rootDir\n",
    "        self.fileList = os.listdir(rootDir)        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.fileList)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = np.load(os.path.join(self.rootDir, self.fileList[index]))\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# dataset class\n",
    "##########################\n",
    "\n",
    "class PianoFluteDataset(Dataset):\n",
    "    def __init__(self, root_piano, root_flute, standardizationStyle):\n",
    "        self.root_piano = root_piano\n",
    "        self.root_flute = root_flute\n",
    "\n",
    "        random.seed(2)\n",
    "        self.piano_images = os.listdir(root_piano)\n",
    "        self.flute_images = sample(os.listdir(root_flute), len(os.listdir(root_flute)))\n",
    "        \n",
    "        self.length_dataset = max(len(self.piano_images), len(self.flute_images))\n",
    "\n",
    "        self.piano_dataset_length = len(self.piano_images)\n",
    "        self.flute_dataset_length = len(self.flute_images)\n",
    "        \n",
    "        self.standardizationStyle = standardizationStyle\n",
    "        \n",
    "        # default to normal for standardizing the data\n",
    "        self.offsetFlute = centerOffset['flute']['normal']\n",
    "        self.offsetPiano = centerOffset['piano']['normal']\n",
    "        \n",
    "        self.divFlute = divFactor['flute']['normal']\n",
    "        self.divPiano = divFactor['piano']['normal']\n",
    "        \n",
    "        if standardizationStyle == 'uniform':\n",
    "            print ('Using uniform assumption...')\n",
    "            self.offsetFlute = centerOffset['flute']['uniform']\n",
    "            self.offsetPiano = centerOffset['piano']['uniform']\n",
    "        \n",
    "            self.divFlute = divFactor['flute']['uniform']\n",
    "            self.divPiano = divFactor['piano']['uniform']\n",
    "            \n",
    "        else:\n",
    "            print ('Using logNormal assumption...')\n",
    "            self.offsetFlute = centerOffset['flute']['logNormal']\n",
    "            self.offsetPiano = centerOffset['piano']['logNormal']\n",
    "        \n",
    "            self.divFlute = divFactor['flute']['logNormal']\n",
    "            self.divPiano = divFactor['piano']['logNormal']\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        flute_image = self.flute_images[index % self.flute_dataset_length]\n",
    "        piano_image = self.piano_images[index % self.piano_dataset_length]\n",
    "\n",
    "        flute_path = os.path.join(self.root_flute, flute_image)\n",
    "        piano_path = os.path.join(self.root_piano, piano_image)\n",
    "        \n",
    "        flute_img = np.load(flute_path)\n",
    "        piano_img = np.load(piano_path)\n",
    "        \n",
    "        # add extra dimension for the single channel\n",
    "        flute_img = flute_img[None, :]\n",
    "        piano_img = piano_img[None, :]\n",
    "        \n",
    "        # standardize the scale\n",
    "        flute_img = (flute_img - self.offsetFlute) / self.divFlute\n",
    "        piano_img = (piano_img - self.offsetPiano) / self.divPiano        \n",
    "\n",
    "        return flute_img, piano_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):    \n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# paths to directories\n",
    "################################\n",
    "\n",
    "curDir = os.getcwd()\n",
    "trainSetDir = f'{curDir}/../../dataSuperSet/processedData/trainSet'\n",
    "fluteTrainSetDir = f'{trainSetDir}/flute/cqtChunks'\n",
    "pianoTrainSetDir = f'{trainSetDir}/piano/cqtChunks'\n",
    "\n",
    "# create directories to store checkpoint outputs\n",
    "checkPointDir = f'{curDir}/checkPoints'\n",
    "checkPointModelDir = f'{checkPointDir}/models'\n",
    "checkPointImageDir = f'{checkPointDir}/images'\n",
    "checkPointLossTrackingDir = f'{checkPointDir}/lossTracking'\n",
    "\n",
    "os.system(f'mkdir -p {checkPointDir}')\n",
    "os.system(f'mkdir -p {checkPointModelDir}')\n",
    "os.system(f'mkdir -p {checkPointImageDir}')\n",
    "os.system(f'mkdir -p {checkPointLossTrackingDir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# datasets\n",
    "################################\n",
    "\n",
    "# create dataset\n",
    "dataset = PianoFluteDataset(pianoTrainSetDir, fluteTrainSetDir, STANDARDIZATION_STYLE)\n",
    "\n",
    "# create dataloader\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# instantiate networks and optimizers\n",
    "#########################################\n",
    "\n",
    "# discriminator piano\n",
    "disc_P = Discriminator(in_channels=1).to(DEVICE)\n",
    "\n",
    "# discriminator flute\n",
    "disc_F = Discriminator(in_channels=1).to(DEVICE)\n",
    "\n",
    "# generator piano\n",
    "gen_P = Generator(img_channels=1, num_residuals = NUM_RESIDUALS).to(DEVICE)\n",
    "\n",
    "# generator piano\n",
    "gen_F = Generator(img_channels=1, num_residuals = NUM_RESIDUALS).to(DEVICE)\n",
    "\n",
    "# optimizer discriminator\n",
    "opt_disc = optim.Adam(list(disc_P.parameters()) + list(disc_F.parameters()), lr = LEARNING_RATE, betas=(ADAM_BETA1, ADAM_BETA2))\n",
    "\n",
    "# optimizer generator\n",
    "opt_gen  = optim.Adam(list(gen_P.parameters())  + list(gen_F.parameters()),  lr = LEARNING_RATE, betas=(ADAM_BETA1, ADAM_BETA2))\n",
    "\n",
    "\n",
    "# Losses\n",
    "\n",
    "# For cycle consistency and identity loss\n",
    "L1 = nn.L1Loss() \n",
    "\n",
    "# adversarial loss\n",
    "mse = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# training\n",
    "################################\n",
    "\n",
    "generatorLossProgression = np.zeros(NUM_EPOCHS)\n",
    "discriminatorLossProgression = np.zeros(NUM_EPOCHS)\n",
    "identityLossProgression = np.zeros(NUM_EPOCHS)\n",
    "cycleLossProgression = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):    \n",
    "    \n",
    "    for idx, (flute, piano) in enumerate(loader):\n",
    "        \n",
    "        # move data to device\n",
    "        piano = piano.to(DEVICE)\n",
    "        flute = flute.to(DEVICE)\n",
    "                \n",
    "        \n",
    "        ##############################\n",
    "        # Discriminator training\n",
    "        ##############################\n",
    "        \n",
    "        # piano generator output \n",
    "        fake_piano = gen_P (flute)        \n",
    "        \n",
    "        # piano discriminator\n",
    "        D_P_real = disc_P(piano)\n",
    "        D_P_fake = disc_P(fake_piano.detach())                \n",
    "        \n",
    "        # real = 1, fake = 0\n",
    "        D_P_real_loss = mse(D_P_real, torch.ones_like(D_P_real))\n",
    "        D_P_fake_loss = mse(D_P_fake, torch.zeros_like(D_P_real))\n",
    "                \n",
    "        \n",
    "        # total piano discriminator loss\n",
    "        Disc_piano_loss = D_P_real_loss + D_P_fake_loss\n",
    "\n",
    "        # flute generator output\n",
    "        fake_flute = gen_F(piano)\n",
    "        \n",
    "        # flute discriminator\n",
    "        D_F_real = disc_F(flute)\n",
    "        D_F_fake = disc_F(fake_flute.detach())\n",
    "        \n",
    "        # real = 1, fake = 0\n",
    "        D_F_real_loss = mse(D_F_real, torch.ones_like(D_F_real))\n",
    "        D_F_fake_loss = mse(D_F_fake, torch.zeros_like(D_F_real))\n",
    "        \n",
    "        # total flute discriminator loss\n",
    "        Disc_flute_loss = D_F_real_loss + D_F_fake_loss\n",
    "\n",
    "        # Overall discriminator loss\n",
    "        D_loss = (Disc_flute_loss + Disc_piano_loss)/2\n",
    "\n",
    "        # zero out the gradients\n",
    "        opt_disc.zero_grad()\n",
    "        \n",
    "        # backprop\n",
    "        D_loss.backward()\n",
    "        \n",
    "        # update discriminator params\n",
    "        opt_disc.step()        \n",
    "        \n",
    "        ##############################\n",
    "        # Generator training\n",
    "        ##############################\n",
    "        \n",
    "        # Adversarial Loss for both generators\n",
    "        D_P_fake = disc_P(fake_piano)\n",
    "        D_F_fake = disc_F(fake_flute)\n",
    "        \n",
    "        # generator wants fake to be detected real\n",
    "        loss_G_F = mse(D_F_fake, torch.ones_like(D_F_fake))\n",
    "        loss_G_P = mse(D_P_fake, torch.ones_like(D_P_fake))\n",
    "\n",
    "        # Cycle Loss\n",
    "        cycle_piano = gen_P(fake_flute)\n",
    "        cycle_flute = gen_F(fake_piano)\n",
    "        cycle_piano_loss = L1(piano, cycle_piano)\n",
    "        cycle_flute_loss = L1(flute, cycle_flute)\n",
    "        \n",
    "        cycle_loss = cycle_flute_loss + cycle_piano_loss\n",
    "\n",
    "        # Identity Loss\n",
    "        identity_flute = gen_F(flute)\n",
    "        identity_piano = gen_P(piano)\n",
    "        identity_piano_loss = L1(piano, identity_piano)\n",
    "        identity_flute_loss = L1(flute, identity_flute)\n",
    "        \n",
    "        identity_loss = identity_flute_loss + identity_piano_loss\n",
    "\n",
    "        # Overall Generator Loss\n",
    "        G_loss = loss_G_F + loss_G_P + cycle_loss*LAMBDA_CYCLE + identity_loss*LAMBDA_IDENTITY\n",
    "\n",
    "        # zero out the gradients\n",
    "        opt_gen.zero_grad()\n",
    "        \n",
    "        # backprop\n",
    "        G_loss.backward()\n",
    "        \n",
    "        # update generator params\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            \n",
    "            # print the current losses\n",
    "            print (f'[epoch = {epoch}, idx = {idx}]:   D_loss = {D_loss.item()}, \\t G_loss = {G_loss.item()}, \\t identity_loss = {identity_loss.item()}, \\t cycle_loss = {cycle_loss.item()}')            \n",
    "            \n",
    "        if idx % 1250 == 0:      \n",
    "            \n",
    "            # save the fake piano and flute np array of a sample            \n",
    "            np.save(f'{checkPointImageDir}/originalPiano__idx_{idx}__{fileTag}.npy', np.squeeze(piano.detach().cpu().numpy()[0,:]))\n",
    "            np.save(f'{checkPointImageDir}/fakeFlute__idx_{idx}__{fileTag}.npy', np.squeeze(fake_flute.detach().cpu().numpy()[0,:]))            \n",
    "            np.save(f'{checkPointImageDir}/originalFlute__idx_{idx}__{fileTag}.npy', np.squeeze(flute.detach().cpu().numpy()[0,:]))\n",
    "            np.save(f'{checkPointImageDir}/fakePiano__idx_{idx}__{fileTag}.npy', np.squeeze(fake_piano.detach().cpu().numpy()[0,:]))\n",
    "        \n",
    "        \n",
    "        # aggregate loss over all batches\n",
    "        discriminatorLossProgression[epoch] += D_loss.item()\n",
    "        generatorLossProgression[epoch] += G_loss.item()\n",
    "        identityLossProgression[epoch] += identity_loss.item()\n",
    "        cycleLossProgression[epoch] += cycle_loss.item()\n",
    "    \n",
    "    # average loss over the length of dataset\n",
    "    discriminatorLossProgression[epoch] = discriminatorLossProgression[epoch] / len(dataset)\n",
    "    generatorLossProgression[epoch] = generatorLossProgression[epoch] / len(dataset)\n",
    "    identityLossProgression[epoch] = identityLossProgression[epoch] / len(dataset)\n",
    "    cycleLossProgression[epoch] = cycleLossProgression[epoch] / len(dataset)\n",
    "    \n",
    "    # save the model and current losses\n",
    "    if SAVE_CHECKPOINTS:\n",
    "        save_checkpoint(gen_P, opt_gen, filename = f'{checkPointModelDir}/genp__{fileTag}.pth.tar')\n",
    "        save_checkpoint(gen_F, opt_gen, filename = f'{checkPointModelDir}/genf__{fileTag}.pth.tar')\n",
    "        save_checkpoint(disc_P, opt_disc, filename = f'{checkPointModelDir}/criticp__{fileTag}.pth.tar')\n",
    "        save_checkpoint(disc_F, opt_disc, filename = f'{checkPointModelDir}/criticf__{fileTag}.pth.tar')\n",
    "    \n",
    "        # save the lossProgressions\n",
    "        np.save(f'{checkPointLossTrackingDir}/discriminatorLossProgression__{fileTag}.npy', discriminatorLossProgression)\n",
    "        np.save(f'{checkPointLossTrackingDir}/generatorLossProgression__{fileTag}.npy', generatorLossProgression)\n",
    "        np.save(f'{checkPointLossTrackingDir}/identityLossProgression__{fileTag}.npy', identityLossProgression)\n",
    "        np.save(f'{checkPointLossTrackingDir}/cycleLossProgression__{fileTag}.npy', cycleLossProgression)\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
